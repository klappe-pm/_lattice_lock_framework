# Multi-Model Consensus Strategy Design

**Task ID:** 6.3.5
**Phase:** 6.3 (Orchestrator Feature Completeness)
**Author:** Gemini Antimatter
**Status:** Approved Design

## 1. Overview
The Multi-Model Consensus Strategy allows the Lattice Lock Orchestrator to leverage multiple LLMs to answer a single prompt, increasing reliability for high-stakes tasks (e.g., security audits, architecture decisions) by aggregating diverse perspectives.

## 2. Core Components

### 2.1 ConsensusStrategy Class
The core logic will be encapsulated in a `ConsensusStrategy` class within `src/lattice_lock_orchestrator/consensus.py` (new file).

```python
from enum import Enum
from dataclasses import dataclass
from typing import List, Optional
from .types import APIResponse, ModelCapabilities

class ReconciliationMethod(Enum):
    MAJORITY_VOTE = "majority_vote"  # Best for classification/multiple choice
    LLM_AS_JUDGE = "llm_as_judge"    # Best for complex reasoning/coding
    MERGE = "merge"                  # Best for creative/list generation tasks
    CONFIDENCE_WEIGHTED = "confidence_weighted" # Use model confidence scores

@dataclass
class ConsensusResult:
    final_answer: str
    individual_responses: List[APIResponse]
    reconciliation_method: ReconciliationMethod
    agreement_score: float  # 0.0 to 1.0
    winning_model: Optional[str] = None
    duration_ms: float = 0.0
    total_cost: float = 0.0

class ConsensusStrategy:
    def __init__(self, 
                 orchestrator: 'ModelOrchestrator',
                 reconciliation: ReconciliationMethod = ReconciliationMethod.LLM_AS_JUDGE,
                 judge_model: str = "gpt-4-turbo", # Default strong model for judging
                 group_size: int = 3):
        self.orchestrator = orchestrator
        self.reconciliation = reconciliation
        self.judge_model = judge_model
        self.group_size = group_size

    async def execute(self, prompt: str, candidates: List[str] = None) -> ConsensusResult:
        """
        Executes the consensus strategy.
        1. Selects models (if candidates not provided)
        2. Parallel executes prompt across models
        3. Reconciles results
        """
        pass
```

### 2.2 Model Selection Logic
If specific models are not requested, the strategy selects `group_size` models based on:
1.  **Capability**: Top scoring models for the task type (using `TaskAnalyzer`).
2.  **Diversity**: Prioritize different providers (e.g., 1 OpenAI, 1 Anthropic, 1 Google) to avoid shared biases or provider-specific failures.

**Selection Algorithm:**
1.  Get all models capable of the task.
2.  Group by Provider.
3.  Round-robin select top model from each provider until `group_size` is reached.
4.  If not enough providers, fill with next best models regardless of provider.

### 2.3 Reconciliation Methods

#### A. Majority Vote (Classification)
*   **Use Case:** Boolean questions, Multiple Choice, Sentinel checks (e.g., "Is this safe?").
*   **Logic:** Normalize answers to canonical forms (Yes/No, Class A/B). Count frequencies.
*   **Tie-breaking:** Highest confidence score or random.

#### B. LLM-as-Judge (Reasoning/Coding)
*   **Use Case:** Code generation, complex explanations.
*   **Logic:**
    1.  Collect all N responses.
    2.  Construct a prompt for the `judge_model`:
        ```text
        You are an expert judge. Evaluate these {N} responses to the following prompt:
        "{original_prompt}"
        
        Response 1 (Model A): ...
        Response 2 (Model B): ...
        ...
        
        Select the best response. Explain your reasoning. 
        If multiple are good, synthesize a better one.
        Return JSON: {{"best_index": int, "synthesis": str, "reasoning": str}}
        ```
    3.  Parse JSON and return result.

#### C. Merge (Creative)
*   **Use Case:** Brainstorming, generating lists (e.g., "List potential security risks").
*   **Logic:** Concatenate unique items from all lists.

### 2.4 Interface Updates
Update `ModelOrchestrator` to support consensus:

```python
# src/lattice_lock_orchestrator/core.py

    async def route_consensus(self, 
                            prompt: str, 
                            method: str = "llm_as_judge", 
                            group_size: int = 3) -> ConsensusResult:
        strategy = ConsensusStrategy(
            orchestrator=self,
            reconciliation=ReconciliationMethod(method),
            group_size=group_size
        )
        return await strategy.execute(prompt)
```

## 3. CLI Integration
Add a new command to `scripts/orchestrator_cli.py` (and later the unified CLI):

```bash
lattice-lock consensus "Explain the difference between TCP and UDP" \
  --count 3 \
  --method llm_as_judge \
  --judge gpt-4
```

**Output View:**
*   Show progress bars for each model execution.
*   Display individual answers (truncated).
*   Show "Judge's Decision" and final synthesized answer.

## 4. Telemetry & Cost
*   **Cost Tracking:** `ConsensusResult` must sum the cost of all individual calls + the judge call.
*   **Latency:** Track total wall-clock time (calls should be `asyncio.gather`ed).
*   **Agreement Score:**
    *   For Voting: % of models agreeing with winner.
    *   For Judge: Heuristic or specific output from judge asking "how similar are these".

## 5. Implementation Tasks (for Devin AI)
1.  **Create `src/lattice_lock_orchestrator/consensus.py`**: Implement `ConsensusStrategy` and Enum.
2.  **Update `src/lattice_lock_orchestrator/core.py`**: Add `route_consensus` method.
3.  **Update CLI**: Add `consensus` command.
4.  **Tests**:
    *   `tests/test_consensus.py`: Mock model responses and verify voting/judgment logic.
    *   Integration test with simulated API calls.

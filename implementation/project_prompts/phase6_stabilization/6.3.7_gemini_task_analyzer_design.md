# Task Analyzer v2 Design

**Task ID:** 6.3.7
**Phase:** 6.3 (Orchestrator Feature Completeness)
**Author:** Gemini Antimatter
**Status:** Approved Design

## 1. Overview
The `TaskAnalyzer` is responsible for understanding user intent to route prompts to the best model. Version 2 improves upon simple keyword matching by introducing a multi-signal classification system (Hybrid Scorer) and supporting multi-label classification (e.g., "Debug this Python code" is both `DEBUGGING` and `CODE_GENERATION`).

## 2. Classification Strategy: Hybrid Signal Processing

We will avoid a full dependency on heavy ML libraries (like torch/transformers) to keep the core lightweight. Instead, we use a **Weighted Signal System**.

### 2.1 Signals
The analyzer extracts multiple signals from the prompt:

1.  **Keyword Density**: Frequency of domain-specific terms (existing but refined).
2.  **Structural Heuristics**:
    *   `has_code_block`: Presence of markdown code blocks or indentation patterns.
    *   `is_question`: Starts with "How/Why/What" or ends with "?".
    *   `length`: Short (<50 chars), Medium, Long (>1k chars).
3.  **Regex Patterns**: Strong indicators (e.g., `def .*\(.*\):` -> Code, `Traceback` -> Debugging).

### 2.2 Task Definitions & Expanded Taxonomy

Update `TaskType` enum in `src/lattice_lock_orchestrator/types.py`:

| Task Type | Definition | Key Signals |
| :--- | :--- | :--- |
| `CODE_GENERATION` | Writing new code or implementing logic. | `write`, `implement`, code blocks |
| `DEBUGGING` | Fixing errors, analyzing stack traces. | `fix`, `error`, `exception`, `traceback`, `fail` |
| `ARCHITECTURAL_DESIGN` | High-level planning, patterns, system design. | `design`, `architecture`, `pattern`, `structure` |
| `DATA_ANALYSIS` | Interpreting data, CSVs, logs. | `csv`, `data`, `analyze`, `trend`, `plot` |
| `TESTING` | Writing or running tests. | `test`, `pytest`, `unit test`, `coverage` |
| `SECURITY_AUDIT` | **(New)** Checking for vulnerabilities. | `security`, `hack`, `vuln`, `safe`, `auth` |
| `CREATIVE_WRITING` | **(New)** Non-technical text generation. | `story`, `poem`, `blog`, `email` |

## 3. Architecture

### 3.1 Interface
The `TaskAnalyzer` should return a comprehensive analysis object.

```python
@dataclass
class TaskAnalysis:
    primary_type: TaskType
    secondary_types: List[TaskType]  # Sorted by confidence
    scores: Dict[TaskType, float]    # Raw scores
    features: Dict[str, Any]         # Extracted features (code_blocks, etc)
    complexity: str                  # "simple", "moderate", "complex"
    min_context_window: int
```

### 3.2 Scoring Logic (Pseudocode)

```python
class TaskAnalyzer:
    def analyze(self, prompt: str) -> TaskAnalysis:
        scores = {t: 0.0 for t in TaskType}
        
        # 1. Regex/Keyword Scoring
        for task_type, patterns in self.patterns.items():
            for pattern, weight in patterns:
                if re.search(pattern, prompt):
                    scores[task_type] += weight
                    
        # 2. Heuristic Boosts
        if self._has_code_blocks(prompt):
            scores[TaskType.CODE_GENERATION] += 0.3
            scores[TaskType.DEBUGGING] += 0.1
            
        if self._has_stack_trace(prompt):
            scores[TaskType.DEBUGGING] += 0.8  # Strong signal
            
        # 3. Normalize & Rank
        # ... logic to determine primary vs secondary ...
        
        return TaskAnalysis(...)
```

## 4. Multi-Label Support
Many tasks overlap. For example, "Write a unit test for this function" is:
*   `TESTING` (Primary, Score: 0.9)
*   `CODE_GENERATION` (Secondary, Score: 0.7)

The `ModelScorer` will be updated to use `TaskAnalysis`. It can now boost models that are good at *both* types, rather than just the primary one.

## 5. Configuration & Extensibility
*   **`config/analyzer_rules.yaml`** (Optional): Allow users to define custom keywords/weights for their specific domain without changing code.
    ```yaml
    rules:
      - task: "SECURITY_AUDIT"
        keywords: ["compliance", "gdpr", "penetration"]
        weight: 0.5
    ```

## 6. Testing Strategy
To ensure accuracy without regression:
1.  **Golden Dataset**: Create `tests/data/analyzer_golden.json` containing pairs of `{ "prompt": "...", "expected_types": [...] }`.
2.  **Regression Test**: `test_analyzer_accuracy.py` runs through the golden set and asserts that the `primary_type` matches expected.
3.  **Threshold Tests**: Ensure "ambiguous" prompts result in lower confidence scores/multiple types.

## 7. Implementation Tasks (for Devin AI)
1.  **Update `types.py`**: Add new `TaskType`s.
2.  **Refactor `scorer.py`**:
    *   Rename/Split: Extract `TaskAnalyzer` to its own file `src/lattice_lock_orchestrator/analyzer.py` (it's currently in `scorer.py`).
    *   Implement Signal-based scoring.
    *   Implement `TaskAnalysis` dataclass.
3.  **Update `ModelScorer.score()`**: logic to consume `TaskAnalysis` (weighted sum of top 2-3 task types).
4.  **Create Golden Test Set**: Add 20-30 varied prompts.

# Cost & Telemetry Strategy Design

**Task ID:** 6.3.3
**Phase:** 6.3 (Orchestrator Feature Completeness)
**Author:** Gemini Antimatter
**Status:** Approved Design

## 1. Overview
This design introduces a persistent telemetry system to track token usage and costs across all model providers. It moves beyond ad-hoc tracking to a structured SQLite-based system.

## 2. Data Model

We will use a flat usage record structure.

```python
from dataclasses import dataclass
from datetime import datetime
from .types import TaskType

@dataclass
class UsageRecord:
    id: str  # uuid
    timestamp: datetime
    model_id: str
    provider: str
    input_tokens: int
    output_tokens: int
    cost_usd: float
    task_type: str  # TaskType.value
    latency_ms: float
    status: str     # "success", "error"
```

## 3. Storage Strategy

**Decision:** **SQLite** (`.lattice/usage.db`).
*   **Rationale:**
    *   **Persistence:** Preserves history across sessions (unlike in-memory).
    *   **Query Power:** Easy to aggregate by day, model, provider using SQL.
    *   **Zero Config:** No running server required (unlike Postgres).
    *   **Portable:** Single file.

**Schema:**
```sql
CREATE TABLE usage_metrics (
    id TEXT PRIMARY KEY,
    timestamp DATETIME NOT NULL,
    model_id TEXT NOT NULL,
    provider TEXT NOT NULL,
    input_tokens INTEGER,
    output_tokens INTEGER,
    cost_usd REAL,
    task_type TEXT,
    latency_ms REAL,
    status TEXT
);
CREATE INDEX idx_timestamp ON usage_metrics(timestamp);
```

## 4. API Surface

The `CostTracker` class manages recording and querying.

```python
# src/lattice_lock_orchestrator/telemetry.py

class CostTracker:
    def __init__(self, db_path: str = ".lattice/usage.db"):
        self._init_db(db_path)

    def record_usage(self, record: UsageRecord) -> None:
        """Insert a record into DB."""
        pass

    def get_summary(self, timeframe: str = "session") -> Dict[str, Any]:
        """
        Get aggregated metrics.
        timeframe: 'session', 'day', 'month', 'all'
        Returns: {
            "total_cost": float,
            "total_tokens": int,
            "by_provider": Dict[str, float],
            "by_model": Dict[str, float]
        }
        """
        pass

    def export_csv(self, path: str) -> None:
        """Dump DB to CSV."""
        pass
```

## 5. Integration

The `ModelOrchestrator` owns a `CostTracker` instance.

**Hook point:** `_call_model` in `core.py`.
1.  Start timer.
2.  Call API.
3.  Stop timer.
4.  Extract usage stats from `APIResponse`.
5.  Calculate cost:
    *   `cost = (in_tokens * cost_per_1k_in / 1000) + (out_tokens * cost_per_1k_out / 1000)`
    *   Pricing data comes from `ModelRegistry`.
6.  Call `tracker.record_usage()`.

## 6. CLI Commands

Update `orchestrator_cli.py` (and the new main CLI):

```bash
# Show session summary
lattice-lock cost

# Show daily report
lattice-lock cost --period day

# Detailed model breakdown
lattice-lock cost --breakdown model

# Export
lattice-lock cost --export usage_2024.csv
```

## 7. Implementation Tasks (Devin AI)

1.  **Create `src/lattice_lock_orchestrator/telemetry.py`**:
    *   Implement `CostTracker` with SQLite backend.
    *   Implement cost calculation helper.
2.  **Update `src/lattice_lock_orchestrator/core.py`**:
    *   Initialize tracker.
    *   Instrument `_call_model` to record data.
3.  **Update `src/lattice_lock_orchestrator/types.py`**:
    *   Ensure `APIResponse` always has `input_tokens` and `output_tokens` (normalize provider responses).
4.  **Tests**:
    *   `tests/test_telemetry.py`: Verify DB writes/reads and aggregations.
    *   `tests/test_cost_calc.py`: Verify math against known pricing.
